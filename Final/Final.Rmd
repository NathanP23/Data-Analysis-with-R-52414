---

title: '52414: Home Exam Questions'

output:

  html_document: default

  pdf_document: default

date: "August 25th, 2024"

---



### Q0.Submission Instructions (Please read carefully)   



The exam will be submitted **individually** by uploading the solved exam `Rmd` and `html` files to the course `moodle`. 

Please name your files as `52414-HomeExam_ID.Rmd` and `52414-HomeExam_ID.html` where `ID` is replaced by your ID number (e.g. `52414-HomeExam_1234.Rmd`. Do **not** write your name in the file name or in the exam itself).





**Grading:** There are $2$ questions with overall $10$ sub-questions. They vary in difficulty and length, and the points for each sub-question are indicated at its beginning. There are in total $100$ points + $4$ bonus points. 



The exam will be available from August 25th on 9:00. The last submission time is at August 28th at 9:00, except for cases that were given approval in advance. <br>

You may use all course materials, the web and other written materials and R libraries. 

You are NOT allowed to discuss any of the exam questions/materials with other students. 





**Analysis and Presentation of Results:**



Write your answers and explanations in the text of the `Rmd` file (*not* in the `code` blocks). <br>

The text of your answers should be next to the relevant code, plots and tables and refer to them, and not at a separate place at the end. <br>

Explain every step of your analysis. When in doubt, a more detailed explanation is better than omitting explanations. 



Give informative titles, axis names and names for each curve/bar in your graphs. 

Change graph limits if needed. If you do so, please include the outlier points you have removed in a separate table.  <br>

Add informative comments explaining your code <br>



Whenever possible, use **objective** and **specific** terms and quantities learned in class, and avoid **subjective** and **general** non-quantified statements. For example: <br>

`Good:` "We see a $2.5$-fold increase in the number of movies from 1970 to 1980". <br>

`Bad:` "The curve goes up at the beginning". <br>

`Good:` "The median is $4.7$. We detected five outliers with distance $>3$ standard deviations from the median". <br>

`Bad:` "The five points on the sides seem far from the middle". 



Sometimes `Tables` are the best way to present your results (e.g. when asked for a list of items). Exclude irrelevant

rows/columns. Display clearly items' names in your `Tables`.



Show numbers in plots/tables using standard digits and not scientific display. 

That is: 90,000,000 or 90M and not 9e+06.  

Round numbers to at most 3 digits after the dot - that is, 9.456 and not 9.45581451044



Some questions may require data wrangling and manipulation which you need to 

decide on. The instructions may not specify precisely the exact plot you should use

(for example: `show the distribution of ...`). In such cases, you should decide what and how to show the results. 



When analyzing real data, use your best judgment if you encounter missing values, negative values, NaNs, errors in the data etc. (e.g. excluding them, zeroing negative values..) and mention what you have done in your analysis in such cases. 



Required libraries are called in the `Rmd` file. Install any library missing from your `R` environment.<br>

Any additional library you want to add must be approved by the course staff. If the approval is given add them at the **start** of the `Rmd` file, right below the existing libraries, and explain what libraries you've added, and what were they used for. 



This is an `.Rmd` file. Copy it with your mouse to create and `Rmd` file, and edit the file to include your questions.



Good luck!







```{r, cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE}


library(wikiTools)

library(httr)

library(jsonlite)

library(data.table)

library(readr)

library(tidyr)

library(pbapply)

library(dplyr)

library(ggplot2)

library(data.table)

library(patchwork)

library(stringr)

library(pracma)

library(reshape2)

```





<br/><br/>





## Q1.  Movies Datasets Analysis



In this question we examine movie data using the free IMDB dataset. See  [here](https://developer.imdb.com/non-commercial-datasets/) for the data documentation, and [here](https://datasets.imdbws.com/) a link for downloading the data files. 

We will use the files `name.basics.tsv`, `title.basics.tsv` and `title.ratings.tsv`. Look 

at the files and read the questions to decide which data files are needed for each question.

In addition, download the file `additional_files.zip` from the `moodle`, which contains additional data files used in the test. 





(a)  (7 pt) Load the needed data for all questions, show the first five rows of each of the three datasets, and briefly explain what you see. Then, find out how many movies are in the data, and identify the newest and oldest movies by `StartYear`. (If there is more than one in each category, display the longest one).  <br>

**Remark:**  To load large tab-delimited data files, use the `fread` command  



(b) (8 pt)  Filter the movies data file to include only movies released between 1970 and 2020 (inclusive). For each year, keep only the `movies` that have ratings and received at least `5` votes. Show the normalized distribution  of the number of movies (i.e. the relative frequency) in each year before and after filtering side by side. Are the distributions similar? 



(c) (8 pt)  Show the average movie runtime over the years for: all movies, the two genres with the most movies, and additionally for the genres `Horror`,`Action,Crime,Drama` (all after filtering of 1.b). Describe the results. 





(d) (8 pt) Load the `sequal_data.csv` data file. This file was automatically generated to contain movies and their sequels. 

- The file has some errors, i.e. incorrect assignment of movies and their sequels. Identify and report at least two such errors and fix them manually. <br>

- Next, show the first five rows of the data. Find and display the movie series with the most sequels. 

- Compute and display the distribution of the differences in time between the first movie and the last movie in the various series. Explain the results.



(e) (8 pt)  There is a common belief that sequels are worse than original movies, often discussed in articles like those on [Stuyvesant's site](https://stuyspec.com/ae/film/the-problem-with-sequels) and [The Odyssey](https://www.theodysseyonline.com/7-reasons-sequels-suck). This is thought to be related to the "Sophomore Slump," where the second attempt fails to match the success of the first. We want to check if second movies really are worse. <br>

Plot in a graph the distribution of IMDB ratings for first movies and second movies in the sequel data (ignore third and later movies in each series), and all movies from IMDB data (after the filtering in 1.b.). Do you see differences between the distributions? do they support the hypothesis that sequels are worse? 



(f) (14 pt)  Load the `tconst_values_for_2d.csv` file and the relevant IMDB file, then merge them based on the `tconst` column. Next, loop on all movies and for each `primaryTitle` from the merged dataset extract the wikipedia `html` file of the movie using the `read_movie_html` function given below. 

Next, extract the introduction of each movie (the first paragraph appearing after the title, see for example marked in red for the movie `The Music Lovers`). <br>

Clean the text by removing non-english letter symbols and common stop words. Identify the five most frequent words remaining after the cleaning process and explain what these words reveal about the data. <br>

**Remark** - Running the code directly should take about 15 minutes, but it will be quicker with a more efficient approach. We recommend after running a `read_movie_html` to save the data in csv and then read it locally to save running time.



(g) (14 pt) Compute a 26 x 26 table of the frequency of consecutive letter pairs in the English language from the text of all movie introductions you collected earlier in 1.g. That is, for example for the text `to be or not to be` the frequencies of the pairs will be: 

`{to:2, be:2, or:1, no:1, ot:1}` and zero for all other letter pairs.

When calculating the letter pairs distribution, use the text without stop words and ignore letter case.

 Present the table in a heatmap, and in addition report the top 5 pairs of letters along with their **relative** frequencies. <br>

- Finally, we want to test the hypothesis that two consecutive letters are independent. Perform a chi-square test using the 26 X 26 table 

comparing the *observed* frequency to the *expected* frequency under the null hypothesis of indepndence. Compute the test statistic, the degrees of freedom and the resulting p-value. Do you reject the null hypothesis? explain. 





```{r}

read_movie_html <- function(input_text) {

  input_text_formated <- str_replace_all(input_text, " ", "_")

  url <- paste0("https://en.wikipedia.org/wiki/", input_text_formated)

  intro <- tryCatch({

    page <- read_html(url)

    temp_var <- page %>% html_nodes("p") %>% .[2] %>% html_text(trim = TRUE)

    temp_var <- str_squish(temp_var)

    temp_var

  }, error = function(e) {

    return(NULL)

  })

  return(temp_var)

}



```







## Q2. Birthday Paradox



(a) (7 pt) Load the `data_birthday.csv` file, which includes birthdays for some people listed in the IMDB dataset. Display the first few rows. Next, find and display the year with the highest number of birth (including the number in this year), and similarly the month with the highest number of birth, and the day of month with the highest number of births.



(b) (12 pt) Here weâ€™ll investigate the [birthday paradox](https://en.wikipedia.org/wiki/Birthday_problem), which suggests that in a random group, the probability of finding at least two people with the same birthday is higher than we might intuitively expect. 



- First, derive a combinatorial formula for the probability $b_n$ of two individual sharing a birthday among a group of $n$ individuals where each birthday $D_i$ is sampled from the uniform probability distribution $D_i \sim U[1,..,365]$ independently. 



- Next use the data from `data_birthday.csv` to compute the empirical probability distribution of birthdays along the year, i.e. the probability $p_j$ of a birthday occurring in any day $j$ of the $365$ days of a year. 

Show this probability distribution in a bar plot. 



- Next, for each group size $n$ from 2 to 100, sample 10,000 times $n$ birthdays from this distribution and compute the proportion of samples where at least one pair of individuals shares the same birthday. Then, plot a graph comparing the theoretical probability distribution according to the uniform distribution with the empirical proportion in the simulation results, both shown as a function of $n$. 

Do the theoretical curve match the simulated one? why? explain. <br>

**Remark:**  You should be able to complete all the simulations in under 5 minutes. If you're unable to do so, you can run 1000 simulations instead, but this will only earn you partial credit (this also applies for other sub-questions requiring 10,000 simulations).



(c) (14 pt) Define $S_{n,k}$ to be the random variable counting the number of groups of size $k$ out of $n$ individuals that all share the same birthday. For example, $S_{n,k}$ is the number of pairs with the same birthday, and in the previous sub-question we computed and estimated $P(S_{n,2}>0)$ as a function of $n$.



- Run 10,000 simulations, sampling each time $n=100$ individuals from the distribution $p_j$ computed in 2.b., and in each simulation record the total number of pairs sharing a birthday. 

Display for $n=100, k=2$ the empirical distribution of $S_{100,2}$. <br>

**Bonus - 2pt:** Repeat the simulation above but this time recording and displaying the empirical distribution of $S_{100,3}$, i.e. number of triplets sharing a birthday. Describe the results. 



- Let $E_{n,k} = E[S_{n,k}]$ be the *expected* number of groups of size $k$ sharing a birthday. 

Suppose that each birthday $D_i$ has a categorical distribution, with $P(D_i=j) = p_j$ such that $p_j$ is the probability of being born at day $j$, and $\sum_{j=1}^{365} p_j=1$.

- Derive a combinatorial formula for $E_{n,k}$ as a function of $n,k$ and the probabilities $p_j$ (**hint:** use linearity of expectation).



- Compute an estimate for the expectation of the number of pairs having the same birthday ($k=2$) using 10,000 simulations, for $p_j$'s given by the empirical distribution you have computed for the `data_birthday.csv` data, for $n$ running from $2$ to $100$. 

Plot: (i) the theoretical expectation with the same $p_j$'s, (ii) the theoretical expectation with the uniform distribution over $365$ values, and (iii)  the empirical expectation form the simulation, all on the same graph as a function of $n$. Do the curves match? Explain. 

What is the smallest $n$ (number of individuals) such that the expectation $E_{n,2}$ is greater than $1$ for each of the three methods? <br>

**Bonus - 2pt:** Repeat the above for $k=3$, i.e. the number of triplets sharing a birthday. What is the smallest $n$ (number of individuals) such that the expectation $E_{n,3}$ is greater than $1$?











**Solutions:**

<Add here your text and code blocks:> 

text

```{r}

# code

```



text



```{r}

# code

```



text

```{r}

# code

```

text

...



