---
title: "52414 - lab 2"
author: '52414'
date: "4/7/2024"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
  pdf_document: default
editor_options:
  markdown:
    wrap: 72
  chunk_output_type: inline
---

# *Lab 2: Text analysis, Sampling and inference*

<br/><br/>

### Submission Instructions

-   This lab will be submitted in pairs (if you don't have a pair,
    please contact us) via the submission link in moodle, by July 25th.
    23:59.

-   Your final submission should include two files: an `Rmd` file (with
    your answers filled-in) and an `html` file that was generated
    automatically by knitting the `Rmd` file using knitr. Name your
    files as `<ID1>_<ID2>.Rmd` and `<ID1>_<ID2>.html` (insert your ID
    numbers instead).

-   The `Rmd` file should work without problems, only with approved
    libraries, and without the need to read local files (!). **Incorrect
    file names and formats will result in a 5 point deduction.**

-   **Grading:** There are $4$ questions with overall $17$
    sub-questions. Each sub-question is worth $6$ points to the overall
    lab grade (total: $102$ points). The questions vary in length and
    difficulty levels. It is recommended to start with the simpler and
    shorter questions. Points may be reduced for incorrect naming of
    files, missing parts and problems in knitting the `Rmd` file and
    general appearance of the report.

-   **Libraries:** The only allowed libraries are listed below (**do not
    add additional libraries without permission from the course
    staff**):

```{r, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(tidyverse) # This includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(reshape2) # melt: change data-frame format long/wide
library(rvest)
library(lubridate)
library(ggridges)
library(corrplot)
library(wordcloud2)
library(moments)
library(spatstat.geom)
library(cowplot)
library(stringr)
library(pbapply)
```

<br/><br/>

## Analysis of Filing Patent Dataset

The [PatentView](https://patentsview.org/) website hosts worldwide data
on patents filling around the globe. We will focus on analyzing patent
filings across different countries, exploring different types of
patents, and conducting comparisons and basic statistical analyses using
additional data.

### General Guidance

-   Your solution should be submitted as a full report integrating text,
    code, figures and tables. For each question, describe first in the
    text of your solution what you're trying to do, then include the
    relevant code, then the results (e.g. figures/tables) and then a
    textual description of them.

-   In most questions the extraction/manipulation of relevant parts of
    the data-frame can be performed using commands from the `tidyverse`
    and `dplyr` R packages, such as `head`, `arrange`, `aggregate`,
    `group-by`, `filter`, `select`, `summaries`, `mutate` etc.

-   When displaying tables, show the relevant columns and rows with
    meaningful names, and describe the results.

-   When displaying figures, make sure that the figure is clear to the
    reader, axis ranges are appropriate, labels for the axis, title and
    different curves/bars are displayed clearly (font sizes are large
    enough), a legend is shown when needed etc. Explain and describe in
    text what is shown in the figure.

-   It could be that in some cases data are missing (e.g. `NA`). Make
    sure that all your calculations (e.g. taking the maximum, average,
    correlation etc.) take this into account. Specifically, the
    calculations should ignore the missing values to allow us to compute
    the desired results for the rest of the values (for example, using
    the option `na.rm = TRUE` or `us = "complete.obs"`).

-   In some questions, especially those involving large datasets, it may
    be beneficial to utilize the `data.table` library, which is capable
    of handling large data relatively quickly. For further reading, you
    can refer to their documentation.

-   Chat GPT and it's friends the language models are great tools! But
    note that they can sometimes mislead and make you go down a very
    unoptimal way, we recommend thinking about what you want,
    considering other offers and researching independently. **Don't use
    them blindly**.

### Questions:

#### 1. **Loading data and basic processing:**

##### a. Load the data in CSV format from the moodle file named 'merged_data_lab2.zip'. Modify all the date-related columns to a `date` format. <br> Additionally, read the data regarding the population over the years, which can be downloaded from World Bank website [here](https://data.worldbank.org/indicator/SP.POP.TOTL?most_recent_year_desc=true). - Also, download the first table that appears in the following [link](https://www.iban.com/country-codes), which will help you convert the country codes to their full names. <br> Display the first five rows of each of the three tables to verify that the loading was done as required. <br> Consider changing the format of the population data to make it more convenient to work with (from wide to long. This is recommended but not mandatory).

##### b. Display in a new table the five newest and five oldest patent requests. For each patent keep only the record with the first inventor (`inventor_sequence`=0). Display the following columns: `patent_id`, `filing_date`, `patent_title`, `Country`. <br>

##### Next, repeat the same operation, but this time display the five cities with the most patents. Here do not filter by the first inventor listed; Instead, consider all inventors listed on the patent to be counted in their city.

#### 2. **Exploratory Data Analysis**<br>

##### a. Using the population table for each year loaded in the previous question, create a new data-frame that contains columns about the `country` (full name, not 2 letter code), `year`, `patents per million`, and `inventors per million`, such that for each country and each year, there will be a row representing the number of unique patents and inventors in this country and year. Display the first few rows of that data set when sorted by country and year (in lexicographic order). <br>

##### b. Display in a graph the number of patents between the years 2015-2019 for the five countries with the most inventions in 2019, and additionally for the five countries with the most inventions in 2015 (all on the same graph). Which countries had the most inventions in both years? What changed between the two groups of countries? <br>

Repeat and display the same graph and analysis also for the number of
inventors per million residents.

##### c. GAMFA or Grandpa? - <img src="https://www.bigodino.it/wp-content/uploads/2017/01/Pixar-Disney-Company-Up-movie.jpg" alt="image" style="width:50px;height:50px;"/> <br>

##### In this question, we will visually present the number of patents and compare them to [GAMFA companies](https://en.wikipedia.org/wiki/Big_Tech). Create a new column that contains the country name of the company that assigned the patent (like `disambig_country`), but if the company is from GAMFA, change its country name to a new country named `GAMFA`.<br> Show a plot like in 2.b of the number of patents in the years 2015-2019 (not normalized) of the `GAMFA` country and six additional countries selected such that the total numbers of unique patents in these countries for 2019 are the closest to `GAMFA` (compared to other countries). You are required to search for the `GAMFA` companies according to the following `disambig_assignee_organization`: - Apple - Amazon - Microsoft - Meta - AlphaBet <br>

```         
**Remarks**<br>
```

###### 1. Check that the company names are correct and exercise judgment, consider both lower and upper-case naming, and correct the search when necessary (explain!) <br>

###### 2. If you have trouble implementing the code and it takes a long time, consider using the `data.table` library, which is much more efficient for large data. Alternatively, you can narrow down the data to only the year 2019, but this will result in a deduction of some points.

##### d. We would like to ask if different types of patents take different time to get approved. <br>

##### Display the distribution of differences between the `filing_date` (the date of patent application filing) and `patent_date` (the date when the patent was registered and accepted) for all patents in each of the `cpc_section` categories (one graph with a seperate curve for each category). Use the `geom_density_ridges_gradient` graph type in `ggplot`. You may use the dictionary found in `g_cpc_current` under the relevant column to convert the code into meaningful patent category names for display ([link](https://patentsview.org/download/data-download-dictionary)). <br>

##### e. Display a heat map of the following countries, in this order, where on the x-axis are the different `cpc_section` categories, on the y-axis the country name, and in color the percentage of inventions in that category out of the total [unique]{style="color: red;"} patents in this country. What do you see? What does this order indicate? Why is it important to look at the countries in this order?

```{r}
selected_countries <- c("Japan", "South Korea", "China", "Taiwan", "Singapore", "Malaysia", "Poland", "Ukraine", "Romania", "Bulgaria", "Belarus", "Brazil", "Argentina", "Colombia", "Chile", "Uruguay")
```

##### f. Is the distribution of patent categories independent of the country?

##### Use the table you've built in 2.e to test this hypothesis. Use the chi square test to test the claim on the entire table, if there are no values in a certain cell, enter the value 0. Just a reminder, the statistic for the test is $T=\sum_{i,j}\frac{\left(O_{ij}-E_{ij}\right)^{2}}{E_{ij}}\sim\chi^{2}_k$ where in our case, $O_{ij}$ is the observed percentage in category $j$ at country $i$, $E_{ij}$ is the expected percentage (you should derive it!), and $k$ the degrees of freedom (determined by the table size).<br> **Remark:** Do not use a chi-squared test function from library! You need to perform the calculations independently and explain both the calculations and the results in detail. An answer without thorough explanations will not receive any points. <br> \##### g. **Bonus (3 points)** - Display as a bar plot, not normalized, the top-10 cities in terms of the number of patents during 2015-2019 in Israel. Which city has the most patents? Does the result surprise you (other than the existence of the city that raises concerns about the validity of the data)? Why do you think this happened? Investigate this and try to infer why.

#### 3. **Basic Text Operations:** <br>

##### a. Calculate the frequency of each word in the `patent_title` column. Remove short common words (e.g. 'is', 'an' ..) called `stop words` using the `tidytext` package, and exclude words containing special characters (such as &, #, /, , etc.). <br>

##### Utilize `wordcloud2` to display the top 100 most frequent words among the remaining words. <br> Additionally, for each patent category compute the `ratio` of the frequency in the category vs. the frequency in the entire text. Filter out words that had less then `200` occurrences in the category. Present in subplots the five words with the highest ratio in each patent category (after filtering). Does the result surprise you? <br> \##### b. Calculate the frequency of each of the 26 letters in the English language (excluding special characters, considering both uppercase and lowercase letters). Present the frequencies sorted after normalization in a bar plot, i.e. the relative frequencies $p_i = \frac{n_i}{n}$, where $n_i$ is the number of occurrences of the $i^{th}$ letter, and $n$ is the total number of letters. <br>

##### c. Find all patents where the first word in `patent title` is `"Method"` and contains either `"machine learning"` or `"neural net"` (or both). Display the distribution of these patents within the `cpc_section` using a normalized bar plot. <br>

Next, identify and report the five most frequent words appearing as the
`first` word in the patent title, together with their counts. Repeat for
the five most frequent words appearing as the `last` word in the patent
title. Are there overlapping words? **Remark:** It is recommended to use
`stringr`, which is quite efficient.

##### d. Suppose that Elon Musk and Mark Zuckerberg randomly choose two words from all the `patent_title` columns. What is the probability that they will choose the same word? <br> First, calculate this probability theoretically, i.e. in a text containing $n$ different words such that $c_i$ is the number of times that word $i$ appears ($i=1,..,n$) represent this probability as a function of the $c_i$'s (derive and write the formula), and then compute the $c_i$ values in the `patent_title` text and compute the actual probability using the formula you have derived. <br>

Next, perform a simulation of 10,000 repetitions where two words are
sampled from the text to estimate this probability empirically and
compare to the theoretical result. <br> What would the probability of
choosing the same word be if the two selected words uniformly from list
of `unique` words? Did the probability increase or decrease? Explain.
<br>

#### 4. **Autocorrection:** <br>

::: {.tenor-gif-embed data-postid="15009048" data-share-method="host" data-aspect-ratio="1.77778" data-width="50%"}
<a href="https://tenor.com/view/monkey-computer-not-working-computer-broken-computer-problems-computer-issues-gif-15009048">Monkey
Computer Not Working GIF</a> from
<a href="https://tenor.com/search/monkey-gifs">Monkey GIFs</a>
:::

```{=html}
<script type="text/javascript" async src="https://tenor.com/embed.js"></script>
```
<br> **General:** For all sub-questions of this question, treat all
words as lowercase. Additionally, we recommend testing your functions
with a few examples to ensure they work correctly, as running them on
the full text can be time-consuming.

##### a. Misspelling -. Write a function that takes a word and randomly selects `c` letters (input for this function) from the word and changes those letters to another letter. The change to another letter should be as follows: replace the letter with another letter selected uniformly at random among all letters adjacent to it on the keyboard. <br>

You may use the list in the solution code template for 4.a at the
bottom. <br> Next, apply the function to the word `probability`, with
$10$ different values of `c` from 1 to 10. Display the original word
together with the `10` misspelled words below it to verify that the
function works as expected.

##### b. Autocorrection1 - Let's correct typos automatically.

Write a function that receives as input a word and a dictionary with a
frequency for each word [(with stop words!)]{style="color: red;"}. The
function searches the dictionary for all words similar to that word,
with a difference of most one letter between the two. Then, the function
returns the three words with the highest frequencies among the similar
words, sorted by frequency. If there are less then three similar words,
replace the missing words with `null`. <br> Select 10 random words
randomly from the `patent_titles` column text (as in 3.d, non-unique
part), and modify one letter in each using the first function above with
`c=1`. <br> Next, run the correction function on each of the 10 words,
using the dictionary with frequencies calculated from the
`patent_titles` column as calculated in question 3.a, but this time keep
all the stop words (do not filter them out). Finally, display a table
with a row for each origin word, its garbled word, and the three
correction suggestions together with their frequencies. For example, if
we received the word: `Werd`<br> We will search for all four-letter
words identical to it except for at most one letter, and display the
three highest frequency words among all the matching words in a row:

```{r, echo=FALSE}
df <- data.frame(
  original_word = "Herd",
  garbledWord = "Werd",
  FirstSuggestion = "word (95)",
  SecndSuggestion = "wird (64)",
  ThirdSuggestion = "herd (30)",
  stringsAsFactors = FALSE
)
df
```

In this example, the autocorrection function will prefer to correct
`werd` to `word` (having the highest frequency of $95$ in the
dictionary), even-though the original correct word was `herd` (having a
lower frequency of $30$).

c.  Autocorrection2 - We will try another correction method that
    utilizes the keyboard structure. Write a new function that takes as
    input the distorted word and a dictionary with frequencies as in
    4.a., and additionally the table of adjacent keyboard letters. The
    function returns the top three suggested corrections as in 4.b.,
    except that we allow only single letter changes that are adjacent in
    the keyboard. For example, the misspelled word `werd` can be changed
    in the 2nd place only to `wwrd`, `wrrd`, `wsrd`, `wdrd`, `wfrd`
    (none of them appears in the dictionary) but not to `word`. <br> Run
    the function and show the results in a table as we did in 4.b. for
    10 random words from the `patent_titles` column.

d.  Putting it all together - Combine all the functions you have built
    to create a comprehensive function that takes as input a sentence
    and a correction method (according to 4.b. or 4.c), alters a single
    letter in each of the sentence words, and attempts to correct them.
    The function will finally return an integer $n_{i}$ representing the
    number of correct word corrections. <br> Demonstrate the
    implementation of the entire process for the sentence: <br>
    `To be or not to be, this is the question`, with `c=1` letter change
    per word, ensuring to print the sentence at each step (in a clear
    and convenient way for the reader!). <br> **Remark:** We recommend
    making an Input flag so that the function prints the steps according
    to the user's choice

e.  Assessment - We will now assess the effectiveness of the two
    auto-correction methods. Randomly select 2,000 titles from
    `patent_titles` column. Use the function from 4.c to distort the
    words with a single letter misspelled in each word and try to
    correct them with each of the methods from 4.b. and 4.c. <br>
    Present the results in a table showing the mean of percentage of
    successfully corrected words for each method. <br> Explain the
    results: Does using the keyboard structure improve the correctness
    performance? <br> ***Remark:*** You can utilize the `pbsapply`
    function to monitor the progress of the process and assess the
    adequacy of the implementation. Under Relatively direct
    implementation a run on all 2,000 sentences may require less than an
    hour to run on a standard computer.</span>

f.  **Bonus (3 points)** : Think about and suggest your own method for
    autocorrection. Show in an assessment as in 4.d. that it improves
    upon the current methods. <br> **Mey the aotucorreect be on you're
    sydeeee!**

**Good luck!**

<br>

**Solution:** (Fill code, text, plots etc.)

Loading the data via URL connection:

# Answers:

## 1.

### 1.a.

#### Code explanation and execution:

> The code sets the working directory using `setwd()` and loads a CSV
> file `merged_data_lab2.csv` into a DataFrame `df` using `read_csv()`.
> It converts the columns `patent_date` and `filing_date` to `Date`
> format using `mutate()` and `as.Date()`. The code also reads
> `total_population.csv` into `total_population`, renames its columns to
> replace periods and remove leading `X` characters using `rename_all()`
> and `str_replace_all()`, and processes the `country_codes` CSV
> similarly. It prints the first five rows of `df`, `total_population`,
> and `country_codes` to confirm data loading using `head()`.

```{r Question_1_a, cache=TRUE}
library(dplyr)
library(readr)
library(stringr)

# Set working directory
setwd("~/Documents/HUJI/Data Analysis with R 52414/Lab2")

# Unzip and load the main data
unzip("data/merged_data_lab2.zip", exdir = "data")
df <- read_csv("data/merged_data_lab2.csv")#, n_max = 100)

# Convert date columns to Date format
df <- df %>%
  mutate(
    patent_date = as.Date(patent_date),
    filing_date = as.Date(filing_date),
  )

# Load population data
total_population <- read_csv("data/total_population.csv", skip = 3) %>%
  rename_all(~ str_replace_all(., "\\.", "_")) %>%
  rename_all(~ str_replace_all(., " ", "_")) %>%
  rename_all(~ str_remove(., "^X"))

# Load country codes data
country_codes <- read_csv("data/country_codes.csv") %>%
  rename_all(~ str_replace_all(., "\\.", "_")) %>%
  rename_all(~ str_replace_all(., " ", "_")) %>%
  rename_all(~ str_replace_all(., "-", "_"))


print_df_info <- function(df_name) {
  cat("Data frame name:", deparse(substitute(df_name)), "\n")
  cat("Column names:\n")
  print(colnames(df_name))
}

print_df_info(df)
print_df_info(total_population)
print_df_info(country_codes)

# Display the first five rows of each table
print(head(df, 5))
print(head(total_population, 5))
print(head(country_codes, 5))
```

#### Analysis of results:

> Here we only get the first 5 rows of each table to verify that the
> different DataFrames were loaded correctly.

### 1.b.

#### Code explanation and execution:

> The code uses `dplyr` to filter `df` for `inventor_sequence` equal to
> 0, sorts by `filing_date`, and selects columns `patent_id`,
> `filing_date`, `patent_title`, and `disambig_city`. It extracts the
> five oldest and newest patents into `result_1b`. For city-level
> analysis, it groups by `disambig_city`, counts patents with
> `summarise(n())`, sorts in descending order, and selects the top five
> cities in `top_five_cities_1b`.

```{r Question_1_b, cache=TRUE}
library(dplyr)

# Filter for the first inventor (inventor_sequence = 0)
df_filtered_1b <- df %>% filter(inventor_sequence == 0)

# Sort by filing_date to get the newest and oldest patents
df_sorted_1b <- df_filtered_1b %>% arrange(filing_date)

# Select specific columns and display the five newest and five oldest patent requests
result_1b <- df_sorted_1b %>%
  select(patent_id, filing_date, patent_title, disambig_city) %>%
  slice(c(1:5, (n()-4):n()))

print(result_1b)

# Group by city and count the number of patents
top_five_cities_1b <- df %>%
  group_by(disambig_city) %>%
  summarise(patent_count = n()) %>%
  arrange(desc(patent_count)) %>%
  slice(1:5)

print(top_five_cities_1b)
```

#### Analysis of results:

> Nothing too special yet, just the first 5 rows of the data showing the
> oldest and newest patents and the top 5 cities with the most patents
> (Tokyo, Seoul, Beijing, Yokohama, San Jose)

## 2.

### 2.a.

#### Code explanation and execution:

> The code merges `total_population` with `country_codes` using
> `inner_join()` to add full country names and 2-letter codes, storing
> the result in `merged_population_2a`. It aggregates `df` by country
> and year, counting unique patents using `n_distinct(patent_id)` and
> unique inventors using `n_distinct(inventor_id)`, and stores this in
> `df_aggregated_2a`. The code pivots `merged_population_2a` to a long
> format using `pivot_longer()`, creating `population_long_2a`. Finally,
> it merges `df_aggregated_2a` with `population_long_2a` using
> `inner_join()` to calculate patents and inventors per million people
> using `mutate()`, storing the result in `final_df_2a`.

```{r Question_2_a, cache=TRUE}
# Load necessary libraries
library(dplyr) # For data manipulation
library(tidyr) # For data tidying

# Step 1: Merge total_population with country_codes to get the full country names and 2-letter codes
merged_population_2a <- total_population %>%
  select(Country_Name, Country_Code, starts_with("19"), starts_with("20")) %>% 
  # select(Country_Name, Country_Code, starts_with("19"), starts_with("20")): 
  # This selects specific columns from the total_population data frame:
  # Country_Name and Country_Code are directly selected.
  # starts_with("19") and starts_with("20") select all columns that start with "19" or "20", representing years from 1960 to 2015.
  
  inner_join(country_codes, by = c("Country_Code" = "Alpha_3_code"))
  # inner_join(country_codes, by = c("Country_Code" = "Alpha_3_code")): 
  # This merges total_population with country_codes on the matching 3-letter country codes.
  # by = c("Country_Code" = "Alpha_3_code") specifies the columns to use for the join: Country_Code from total_population and Alpha_3_code from country_codes.

# Step 2: Aggregate df by country and year to count unique patents and inventors
df_aggregated_2a <- df %>%
  inner_join(country_codes, by = c("disambig_country" = "Alpha_2_code")) %>% 
  # inner_join(country_codes, by = c("disambig_country" = "Alpha_2_code")): 
  # This merges df with country_codes to add the full country names and 3-letter country codes using the 2-letter country codes.
  # by = c("disambig_country" = "Alpha_2_code") specifies the columns to use for the join: disambig_country from df and Alpha_2_code from country_codes.
  
  mutate(year = as.numeric(format(as.Date(patent_date), "%Y"))) %>% 
  # mutate(year = as.numeric(format(as.Date(patent_date), "%Y"))): 
  # This extracts the year from the patent_date column and converts it to a numeric type.
  # as.Date(patent_date) converts the patent_date to a date object.
  # format(..., "%Y") extracts the year as a string.
  # as.numeric(...) converts the year string to a numeric value.
  
  group_by(Country, year) %>% 
  # group_by(Country, year): 
  # This groups the data by Country and year, allowing for aggregation within each group.
  
  summarize(unique_patents = n_distinct(patent_id), unique_inventors = n_distinct(inventor_id), .groups = 'drop') 
  # summarize(unique_patents = n_distinct(patent_id), unique_inventors = n_distinct(inventor_id), .groups = 'drop'): 
  # This summarizes the data by counting the number of distinct patent_id (unique patents) and inventor_id (unique inventors) within each group.
  # n_distinct(...) counts the number of unique values.
  # .groups = 'drop' removes the grouping after summarizing, returning to an ungrouped data frame.

# Step 3: Pivot total_population to long format to get year-wise population data
population_long_2a <- merged_population_2a %>%
  pivot_longer(cols = starts_with("19") | starts_with("20"), names_to = "year", values_to = "population") %>% 
  # pivot_longer(cols = starts_with("19") | starts_with("20"), names_to = "year", values_to = "population"): 
  # This converts the data from wide format to long format.
  # cols = starts_with("19") | starts_with("20") specifies the columns to pivot: all columns starting with "19" or "20".
  # names_to = "year" specifies that the names of the pivoted columns (the years) will be stored in a new column named "year".
  # values_to = "population" specifies that the values of the pivoted columns will be stored in a new column named "population".
  
  mutate(year = as.numeric(year)) 
  # mutate(year = as.numeric(year)): 
  # This converts the year column from character type to numeric type.

# Step 4: Merge the aggregated patent data with the population data and calculate metrics
final_df_2a <- df_aggregated_2a %>%
  inner_join(population_long_2a, by = c("Country" = "Country_Name", "year")) %>% 
  # inner_join(population_long_2a, by = c("Country" = "Country_Name", "year")): 
  # This merges the aggregated patent data with the long-format population data.
  # by = c("Country" = "Country_Name", "year") specifies the columns to use for the join: Country from df_aggregated_2a and Country_Name from population_long_2a, and year from both data frames.
  
  mutate(
    patents_per_million = unique_patents / (population / 1e6), 
    # patents_per_million = unique_patents / (population / 1e6): 
    # This calculates the number of patents per million people by dividing the count of unique patents by the population (in millions).
    
    inventors_per_million = unique_inventors / (population / 1e6) 
    # inventors_per_million = unique_inventors / (population / 1e6): 
    # This calculates the number of inventors per million people by dividing the count of unique inventors by the population (in millions).
  ) %>%
  select(Country, year, patents_per_million, inventors_per_million) %>% 
  # select(Country, year, patents_per_million, inventors_per_million): 
  # This keeps only the specified columns in the final data frame.
  
  arrange(Country, year) 
  # arrange(Country, year): 
  # This sorts the data frame first by Country and then by year in ascending order.

# Display the first few rows of the final data frame
head(final_df_2a)
# head(final_df_2a): 
# This displays the first few rows of the final data frame for inspection.

write.csv(final_df_2a, "df.csv")
```

#### Analysis of results:

> The data-set provides a snapshot of innovation metrics across various
> countries from 2016 to 2020, focusing on patents and inventors per
> million people. Initial observations reveal substantial variability
> both across countries and over time. For instance, Afghanistan
> exhibits fluctuating rates of patents and inventors, with notable
> peaks in 2018. Such volatility might be attributable to
> socio-political factors or changes in economic policies. Comparing
> countries, those with robust innovation ecosystems, like those in
> North America, Europe, or East Asia, likely display consistently
> higher numbers of patents and inventors per million. This trend
> suggests a strong correlation between economic development, education
> systems, and innovation outputs. Further, the data can illuminate
> regional trends, such as higher innovation rates in technologically
> advanced regions compared to developing ones. Understanding these
> patterns can help policymakers identify successful strategies and
> areas needing support to foster innovation globally.

### 2.b.

#### Code explanation and execution:

> The code aggregates `df` to find the total number of inventions per
> country for 2015-2019 using `group_by()` and
> `summarize(n_distinct(patent_id))`, storing results in
> `df_inventions_2b`. It identifies the top five countries with the most
> inventions in 2015 and 2019 using `filter(year == 2015)` and
> `filter(year == 2019)`, combines these lists using `union()`, and
> filters the data to include only these countries using
> `filter(Country %in% top_countries_combined_2b)`. It creates a graph
> displaying the number of patents for these countries between 2015-2019
> using `ggplot()`, `geom_line()`, and `labs()`. Finally, it analyzes
> which countries had the most inventions in both years and the changes
> between the two groups of countries using `intersect()` and
> `setdiff()`.

```{r Question_2_b, cache=TRUE}
# Load necessary libraries
library(dplyr) # For data manipulation
library(ggplot2) # For data visualization

# Step 1: Aggregate the data to find the total number of inventions per country for the years 2015 and 2019
df_inventions_2b <- df %>%
  inner_join(country_codes, by = c("disambig_country" = "Alpha_2_code")) %>% 
  # Join df with country_codes on 2-letter country codes to add full country names
  
  mutate(year = as.numeric(format(as.Date(patent_date), "%Y"))) %>% 
  # Extract the year from patent_date and convert it to numeric
  
  filter(year >= 2015 & year <= 2019) %>% 
  # Filter the data to include only the years 2015 to 2019
  
  group_by(Country, year) %>% 
  # Group the data by Country and year
  
  summarize(total_inventions = n_distinct(patent_id), .groups = 'drop') 
  # Summarize the data to count the number of unique patents per country per year

# Step 2: Identify the top five countries with the most inventions in 2015 and 2019
top_countries_2015_2b <- df_inventions_2b %>%
  filter(year == 2015) %>%
  # Filter the data to include only the year 2015
  
  top_n(5, wt = total_inventions) %>%
  # Select the top 5 countries with the most inventions in 2015
  
  pull(Country)
  # Extract the country names

top_countries_2019_2b <- df_inventions_2b %>%
  filter(year == 2019) %>%
  # Filter the data to include only the year 2019
  
  top_n(5, wt = total_inventions) %>%
  # Select the top 5 countries with the most inventions in 2019
  
  pull(Country)
  # Extract the country names

# Combine the top countries from 2015 and 2019
top_countries_combined_2b <- union(top_countries_2015_2b, top_countries_2019_2b)
# Create a combined list of unique top countries from both 2015 and 2019

# Step 3: Filter the data to include only these countries for the years 2015-2019
df_filtered_2b <- df_inventions_2b %>%
  filter(Country %in% top_countries_combined_2b)
  # Filter the data to include only the top countries for the years 2015-2019

# Step 4: Create a graph to display the number of patents for these countries between 2015 and 2019
ggplot(df_filtered_2b, aes(x = year, y = total_inventions, color = Country)) +
  # Initialize ggplot with the filtered data, mapping year to x-axis, total_inventions to y-axis, and color by Country
  
  geom_line(size = 1.2) +
  # Add a line geometry to the plot with a specified line size
  
  labs(title = "Number of Patents (2015-2019) for Top Countries",
       x = "Year",
       y = "Number of Patents",
       color = "Country") +
  # Add labels for the title, x-axis, y-axis, and legend
  
  theme_minimal()
  # Apply a minimal theme to the plot for a clean appearance

# Step 5: Analyze which countries had the most inventions in both years and what changed between the two groups of countries
common_countries_2b <- intersect(top_countries_2015_2b, top_countries_2019_2b)
# Find countries that are in the top 5 for both 2015 and 2019

only_2015_2b <- setdiff(top_countries_2015_2b, top_countries_2019_2b)
# Find countries that are in the top 5 in 2015 but not in 2019

only_2019_2b <- setdiff(top_countries_2019_2b, top_countries_2015_2b)
# Find countries that are in the top 5 in 2019 but not in 2015

# Display the results
list(
  common_countries_2b = common_countries_2b,
  only_2015_2b = only_2015_2b,
  only_2019_2b = only_2019_2b
)
# Return a list containing the common countries, those only in 2015, and those only in 2019



# Load necessary libraries
library(dplyr) # For data manipulation
library(ggplot2) # For data visualization

# Step 1: Aggregate the data to find the total number of inventors per million residents per country for the years 2015-2019
df_inventors_2b <- df %>%
  inner_join(country_codes, by = c("disambig_country" = "Alpha_2_code")) %>% 
  # Join df with country_codes on 2-letter country codes to add full country names
  
  mutate(year = as.numeric(format(as.Date(patent_date), "%Y"))) %>% 
  # Extract the year from patent_date and convert it to numeric for filtering
  
  filter(year >= 2015 & year <= 2019) %>% 
  # Filter the data to include only the years 2015 to 2019
  
  group_by(Country, year) %>% 
  # Group the data by Country and year to prepare for summarizing
  
  summarize(total_inventors = n_distinct(inventor_id), .groups = 'drop') 
  # Summarize the data to count the number of unique inventors per country per year

# Merge with total_population data to get population for each country and year
df_population_2b <- total_population %>%
  select(Country_Name, Country_Code, starts_with("19"), starts_with("20")) %>% 
  # Select relevant columns from total_population: Country_Name, Country_Code, and year columns
  
  inner_join(country_codes, by = c("Country_Code" = "Alpha_3_code")) %>% 
  # Join with country_codes on 3-letter country codes to add the full country names and 2-letter codes
  
  pivot_longer(cols = starts_with("19") | starts_with("20"), names_to = "year", values_to = "population") %>% 
  # Pivot the data from wide to long format to have a year-wise population column
  
  mutate(year = as.numeric(year)) 
  # Convert the year column from character to numeric for merging

df_inventors_2b <- df_inventors_2b %>%
  inner_join(df_population_2b, by = c("Country" = "Country_Name", "year")) %>% 
  # Merge the inventors data with the population data on Country and year
  
  mutate(inventors_per_million = total_inventors / (population / 1e6)) 
  # Calculate the number of inventors per million residents

# Step 2: Identify the top five countries with the most inventors per million residents in 2015 and 2019
top_countries_inventors_2015_2b <- df_inventors_2b %>%
  filter(year == 2015) %>% 
  # Filter the data to include only the year 2015
  
  top_n(5, wt = inventors_per_million) %>% 
  # Select the top 5 countries with the most inventors per million residents in 2015
  
  pull(Country) 
  # Extract the country names from the filtered data

top_countries_inventors_2019_2b <- df_inventors_2b %>%
  filter(year == 2019) %>% 
  # Filter the data to include only the year 2019
  
  top_n(5, wt = inventors_per_million) %>% 
  # Select the top 5 countries with the most inventors per million residents in 2019
  
  pull(Country) 
  # Extract the country names from the filtered data

# Combine the top countries from 2015 and 2019
top_countries_inventors_combined_2b <- union(top_countries_inventors_2015_2b, top_countries_inventors_2019_2b) 
# Create a combined list of unique top countries from both 2015 and 2019

# Step 3: Filter the data to include only these countries for the years 2015-2019
df_filtered_inventors_2b <- df_inventors_2b %>%
  filter(Country %in% top_countries_inventors_combined_2b) 
  # Filter the data to include only the top countries for the years 2015-2019

# Step 4: Create a graph to display the number of inventors per million residents for these countries between 2015 and 2019
ggplot(df_filtered_inventors_2b, aes(x = year, y = inventors_per_million, color = Country)) + 
  # Initialize ggplot with the filtered data, mapping year to x-axis, inventors_per_million to y-axis, and color by Country
  
  geom_line(size = 1.2) + 
  # Add a line geometry to the plot with a specified line size
  
  labs(title = "Number of Inventors per Million Residents (2015-2019) for Top Countries",
       x = "Year",
       y = "Number of Inventors per Million",
       color = "Country") + 
  # Add labels for the title, x-axis, y-axis, and legend
  
  theme_minimal() 
  # Apply a minimal theme to the plot for a clean appearance

# Step 5: Analyze which countries had the most inventors per million in both years and what changed between the two groups of countries
common_countries_inventors_2b <- intersect(top_countries_inventors_2015_2b, top_countries_inventors_2019_2b) 
# Find countries that are in the top 5 for both 2015 and 2019

only_inventors_2015_2b <- setdiff(top_countries_inventors_2015_2b, top_countries_inventors_2019_2b) 
# Find countries that are in the top 5 in 2015 but not in 2019

only_inventors_2019_2b <- setdiff(top_countries_inventors_2019_2b, top_countries_inventors_2015_2b) 
# Find countries that are in the top 5 in 2019 but not in 2015

# Display the results
list(
  common_countries_inventors_2b = common_countries_inventors_2b, 
  # List of countries in the top 5 for both 2015 and 2019
  
  only_inventors_2015_2b = only_inventors_2015_2b, 
  # List of countries in the top 5 only in 2015
  
  only_inventors_2019_2b = only_inventors_2019_2b 
  # List of countries in the top 5 only in 2019
)



```

#### Analysis of results:

> the analysis of the results is as follows:

### 2.c.

#### Code explanation and execution:

> The code 

```{r Question_2_c, cache=TRUE}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(data.table)

# Assuming the data frames `df` and `country_codes` are already loaded

# Step 1: Create a new column that contains the country name, but change it to "GAMFA" for GAMFA companies
# Define the GAMFA companies
gamfa_companies <- c("Apple", "Amazon", "Microsoft", "Meta", "Alphabet")

# Create a new column 'assignee_country_gamfa'
df <- df %>%
  mutate(
    assignee_country_gamfa = ifelse(
      tolower(disambig_assignee_organization) %in% tolower(gamfa_companies),
      "GAMFA",
      disambig_country
    )
  )
# Explanation:
# - tolower is used to handle both lower and upper-case naming.
# - ifelse checks if the assignee organization is in the list of GAMFA companies and assigns "GAMFA" if true.

# Step 2: Filter the data for the years 2015-2019
df_filtered_2c <- df %>%
  mutate(year = as.numeric(format(as.Date(patent_date), "%Y"))) %>% # Extract year from patent_date
  filter(year >= 2015 & year <= 2019) # Filter for the years 2015-2019

# Step 3: Aggregate the number of patents by country and year
patents_by_country_2c <- df_filtered_2c %>%
  group_by(assignee_country_gamfa, year) %>% # Group by country and year
  summarize(total_patents = n_distinct(patent_id), .groups = 'drop') # Count unique patents

# Step 4: Identify the top six countries (other than GAMFA) whose total number of unique patents in 2019 are closest to that of GAMFA
# Get the total number of patents for GAMFA in 2019
gamfa_patents_2019 <- patents_by_country_2c %>%
  filter(assignee_country_gamfa == "GAMFA" & year == 2019) %>%
  pull(total_patents)

# Get the total number of patents for each country in 2019
patents_2019 <- patents_by_country_2c %>%
  filter(year == 2019)

# Calculate the absolute difference from GAMFA and arrange by this difference
closest_countries_2019 <- patents_2019 %>%
  filter(assignee_country_gamfa != "GAMFA") %>%
  mutate(diff_from_gamfa = abs(total_patents - gamfa_patents_2019)) %>%
  arrange(diff_from_gamfa) %>%
  head(6) %>%
  pull(assignee_country_gamfa)

# Combine GAMFA and the closest six countries
top_countries_2c <- c("GAMFA", closest_countries_2019)

# Step 5: Filter the data to include only these countries for the years 2015-2019
df_top_countries_2c <- patents_by_country_2c %>%
  filter(assignee_country_gamfa %in% top_countries_2c)

# Step 6: Create a graph to display the number of patents for these countries between 2015 and 2019
ggplot(df_top_countries_2c, aes(x = year, y = total_patents, color = assignee_country_gamfa)) +
  geom_line(size = 1.2) +
  labs(title = "Number of Patents (2015-2019) for GAMFA and Closest Countries",
       x = "Year",
       y = "Number of Patents",
       color = "Country") +
  theme_minimal()

# Display the plot
print(ggplot(df_top_countries_2c, aes(x = year, y = total_patents, color = assignee_country_gamfa)) +
  geom_line(size = 1.2) +
  labs(title = "Number of Patents (2015-2019) for GAMFA and Closest Countries",
       x = "Year",
       y = "Number of Patents",
       color = "Country") +
  theme_minimal())

```

#### Analysis of results:

> the analysis of the results is as follows:

### 2.d.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results:

> the analysis of the results is as follows:

### 2.e.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results:

> the analysis of the results is as follows:

### 2.f.

#### Code explanation and execution:

##### The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results:

> the analysis of the results is as follows:

## 3.

### 3.a.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results:

> the analysis of the results is as follows:

### 3.b.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results: \> the analysis of the results is as follows:

### 3.c.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results:

> the analysis of the results is as follows:

### 3.d.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results:

> the analysis of the results is as follows:

## 4.

### 4.a.

#### Code explanation and execution:

> The code

```{r}
keyboard_neighbors <- list(
  q = c("w", "a", "s"),
  w = c("q", "e", "a", "s", "d"),
  e = c("w", "r", "s", "d", "f"),
  r = c("e", "t", "d", "f", "g"),
  t = c("r", "y", "f", "g", "h"),
  y = c("t", "u", "g", "h", "j"),
  u = c("y", "i", "h", "j", "k"),
  i = c("u", "o", "j", "k", "l"),
  o = c("i", "p", "k", "l"),
  p = c("o", "l"),

  a = c("q", "w", "s", "z"),
  s = c("a", "w", "e", "d", "z", "x"),
  d = c("s", "e", "r", "f", "x", "c"),
  f = c("d", "r", "t", "g", "c", "v"),
  g = c("f", "t", "y", "h", "v", "b"),
  h = c("g", "y", "u", "j", "b", "n"),
  j = c("h", "u", "i", "k", "n", "m"),
  k = c("j", "i", "o", "l", "m"),
  l = c("k", "o", "p"),

  z = c("a", "s", "x"),
  x = c("z", "s", "d", "c"),
  c = c("x", "d", "f", "v"),
  v = c("c", "f", "g", "b"),
  b = c("v", "g", "h", "n"),
  n = c("b", "h", "j", "m"),
  m = c("n", "j", "k")
)

```

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results: \> the analysis of the results is as follows:

### 4.b.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results: \> the analysis of the results is as follows:

### 4.c.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results: \> the analysis of the results is as follows:

### 4.d.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results:

> the analysis of the results is as follows:

### 4.e.

#### Code explanation and execution:

> The code

```{r, cache=TRUE}
# YOUR CODE HERE
```

#### Analysis of results:

> the analysis of the results is as follows:
